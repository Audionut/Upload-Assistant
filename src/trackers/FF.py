# -*- coding: utf-8 -*-
import asyncio
import glob
import httpx
import os
import platform
import re
import unicodedata
from .COMMON import COMMON
from bs4 import BeautifulSoup
from pymediainfo import MediaInfo
from src.console import console
from src.exceptions import UploadException
from src.languages import process_desc_language


class FF(COMMON):
    def __init__(self, config):
        super().__init__(config)
        self.tracker = "FF"
        self.banned_groups = [""]
        self.source_flag = "FunFile"
        self.base_url = "https://www.funfile.org"
        self.torrent_url = "https://www.funfile.org/details.php?id="
        self.announce = self.config['TRACKERS'][self.tracker]['announce_url']
        self.auth_token = None
        self.session = httpx.AsyncClient(headers={
            'User-Agent': f"Audionut's Upload Assistant ({platform.system()} {platform.release()})"
        }, timeout=30.0)
        self.signature = "[center][url=https://github.com/Audionut/Upload-Assistant]Created by Audionut's Upload Assistant[/url][/center]"

    async def validate_credentials(self, meta):
        self.cookie_file = os.path.abspath(f"{meta['base_dir']}/data/cookies/{self.tracker}.txt")
        if not os.path.exists(self.cookie_file):
            await self.login(meta)

        test_url = f"{self.base_url}/upload.php"
        try:
            self.session.cookies.update(await self.parseCookieFile(self.cookie_file))
            response = await self.session.get(test_url, timeout=30)

            if response.status_code == 200 and 'login.php' not in str(response.url):
                return True
            else:
                await self.login(meta)
                response = await self.session.get(test_url, timeout=30)
                if response.status_code == 200 and 'login.php' not in str(response.url):
                    return True
                else:
                    return False
        except Exception as e:
            console.print(f"[bold red]{self.tracker}: Error validating credentials: {e}[/bold red]")
            return False

    async def login(self, meta):
        login_url = "https://www.funfile.org/takelogin.php"
        self.cookie_file = os.path.abspath(f"{meta['base_dir']}/data/cookies/{self.tracker}.txt")

        payload = {
            "returnto": "/index.php",
            "username": self.config['TRACKERS'][self.tracker]['username'],
            "password": self.config['TRACKERS'][self.tracker]['password'],
            "login": "Login"
        }

        print(f"[{self.tracker}] Trying to login...")
        response = await self.session.post(login_url, data=payload)

        if response.status_code == 302:
            print(f"[{self.tracker}] Login Successful!")

            with open(self.cookie_file, "w") as f:
                f.write("# Netscape HTTP Cookie File\n")
                f.write("# This file was generated by an automated script.\n\n")
                for cookie in self.session.cookies.jar:
                    domain = cookie.domain
                    include_subdomains = "TRUE" if domain.startswith('.') else "FALSE"
                    path = cookie.path
                    secure = "TRUE" if cookie.secure else "FALSE"
                    expires = str(int(cookie.expires)) if cookie.expires else "0"
                    name = cookie.name
                    value = cookie.value
                    f.write(f"{domain}\t{include_subdomains}\t{path}\t{secure}\t{expires}\t{name}\t{value}\n")
            print(f"[{self.tracker}] Saving the cookie file...")
        else:
            print(f"[{self.tracker}] Login failed. Status code: {response.status_code}")

    async def search_existing(self, meta, disctype):
        if meta['category'] == 'MOVIE':
            query = meta['title']
        if meta['category'] == 'TV':
            query = f"{meta['title']} {meta.get('season', '')}{meta.get('episode', '')}"

        search_url = f"{self.base_url}/suggest.php?q={query}"
        response = await self.session.get(search_url)

        if response.status_code == 200 and 'suggest.php' in str(response.url):
            items = [line.strip() for line in response.text.splitlines() if line.strip()]
            return items

        return []

    async def get_requests(self, meta):
        if self.config['TRACKERS'][self.tracker].get('check_requests', False) is False:
            return False

        else:
            try:
                category = self.get_type_id(meta)

                query_1 = meta['title']
                query_2 = meta['title'].replace(' ', '.')

                search_url_1 = f"{self.base_url}/requests.php?filter=open&category={category}&search={query_1}"

                if query_1 != query_2:
                    search_url_2 = f"{self.base_url}/requests.php?filter=open&category={category}&search={query_2}"
                    responses = await asyncio.gather(
                        self.session.get(search_url_1),
                        self.session.get(search_url_2)
                    )
                    response_results_text = responses[0].text + responses[1].text
                    responses[0].raise_for_status()
                    responses[1].raise_for_status()
                else:
                    response = await self.session.get(search_url_1)
                    response.raise_for_status()
                    response_results_text = response.text

                soup = BeautifulSoup(response_results_text, "html.parser")
                request_rows = soup.select("td.mf_content table tr")

                results = []
                for row in request_rows:
                    name_element = row.select_one("td.row3 nobr a b")
                    if not name_element:
                        continue

                    name = name_element.text.strip()
                    link_element = name_element.find_parent("a")
                    link = link_element["href"] if link_element else None

                    all_tds = row.find_all("td", class_="row3")
                    reward = all_tds[2].text.strip() if len(all_tds) > 2 else None

                    results.append({
                        "Name": name,
                        "Link": link,
                        "Reward": reward
                    })

                if results:
                    message = f"\n{self.tracker}: [bold yellow]Your upload may fulfill the following request(s), check it out:[/bold yellow]\n\n"
                    for r in results:
                        message += f"[bold green]Name:[/bold green] {r['Name']}\n"
                        message += f"[bold green]Reward:[/bold green] {r['Reward']}\n"
                        message += f"[bold green]Link:[/bold green] {r['Link']}\n\n"
                    console.print(message)

                return results

            except Exception as e:
                print(f"An error occurred while fetching requests: {e}")
                return []

    async def generate_description(self, meta):
        base_desc_path = f"{meta['base_dir']}/tmp/{meta['uuid']}/DESCRIPTION.txt"
        final_desc_path = f"{meta['base_dir']}/tmp/{meta['uuid']}/[{self.tracker}]DESCRIPTION.txt"

        description_parts = []

        # MediaInfo/BDInfo
        tech_info = ""
        if meta.get('is_disc') != 'BDMV':
            video_file = meta['filelist'][0]
            mi_template = os.path.abspath(f"{meta['base_dir']}/data/templates/MEDIAINFO.txt")
            if os.path.exists(mi_template):
                try:
                    media_info = MediaInfo.parse(video_file, output="STRING", full=False, mediainfo_options={"inform": f"file://{mi_template}"})
                    tech_info = str(media_info)
                except Exception:
                    console.print("[bold red]Couldn't find the MediaInfo template[/bold red]")
                    mi_file_path = f"{meta['base_dir']}/tmp/{meta['uuid']}/MEDIAINFO_CLEANPATH.txt"
                    if os.path.exists(mi_file_path):
                        with open(mi_file_path, 'r', encoding='utf-8') as f:
                            tech_info = f.read()
            else:
                console.print("[bold yellow]Using normal MediaInfo for the description.[/bold yellow]")
                mi_file_path = f"{meta['base_dir']}/tmp/{meta['uuid']}/MEDIAINFO_CLEANPATH.txt"
                if os.path.exists(mi_file_path):
                    with open(mi_file_path, 'r', encoding='utf-8') as f:
                        tech_info = f.read()
        else:
            bd_summary_file = f"{meta['base_dir']}/tmp/{meta['uuid']}/BD_SUMMARY_00.txt"
            if os.path.exists(bd_summary_file):
                with open(bd_summary_file, 'r', encoding='utf-8') as f:
                    tech_info = f.read()

        if tech_info:
            description_parts.append(tech_info)

        if os.path.exists(base_desc_path):
            with open(base_desc_path, 'r', encoding='utf-8') as f:
                manual_desc = f.read()
            description_parts.append(manual_desc)

        # Screenshots
        images = meta.get('image_list', [])
        if images:
            screenshots_block = "[center]"
            for image in images:
                img_url = image['img_url']
                web_url = image['web_url']
                screenshots_block += f'<a href="{web_url}" target="_blank"><img src="{img_url}" width="220"></a> '
            screenshots_block += "[/center]"

            description_parts.append(screenshots_block)

        custom_description_header = self.config['DEFAULT'].get('custom_description_header', '')
        if custom_description_header:
            description_parts.append(custom_description_header)

        if self.signature:
            description_parts.append(self.signature)

        final_description = "\n\n".join(filter(None, description_parts))
        from src.bbcode import BBCODE
        bbcode = BBCODE()
        desc = final_description
        desc = re.sub(r'\n{3,}', '\n\n', desc)
        desc = desc.replace("[user]", "").replace("[/user]", "")
        desc = desc.replace("[align=left]", "").replace("[/align]", "")
        desc = desc.replace("[right]", "").replace("[/right]", "")
        desc = desc.replace("[align=right]", "").replace("[/align]", "")
        desc = desc.replace("[sup]", "").replace("[/sup]", "")
        desc = desc.replace("[sub]", "").replace("[/sub]", "")
        desc = desc.replace("[alert]", "").replace("[/alert]", "")
        desc = desc.replace("[note]", "").replace("[/note]", "")
        desc = desc.replace("[hr]", "").replace("[/hr]", "")
        desc = desc.replace("[h1]", "[u][b]").replace("[/h1]", "[/b][/u]")
        desc = desc.replace("[h2]", "[u][b]").replace("[/h2]", "[/b][/u]")
        desc = desc.replace("[h3]", "[u][b]").replace("[/h3]", "[/b][/u]")
        desc = desc.replace("[ul]", "").replace("[/ul]", "")
        desc = desc.replace("[ol]", "").replace("[/ol]", "")
        desc = desc.replace("[hide]", "").replace("[/hide]", "")
        desc = desc.replace("•", "-").replace("“", '"').replace("”", '"')
        desc = re.sub(r"\[center\]\[spoiler=.*? NFO:\]\[code\](.*?)\[/code\]\[/spoiler\]\[/center\]", r"", desc, flags=re.DOTALL)
        desc = bbcode.convert_comparison_to_centered(desc, 1000)
        desc = bbcode.remove_spoiler(desc)

        # [url][img=000]...[/img][/url]
        desc = re.sub(
            r"\[url=(?P<href>[^\]]+)\]\[img=(?P<width>\d+)\](?P<src>[^\[]+)\[/img\]\[/url\]",
            r'<a href="\g<href>" target="_blank"><img src="\g<src>" width="\g<width>"></a>',
            desc,
            flags=re.IGNORECASE
        )

        # [url][img]...[/img][/url]
        desc = re.sub(
            r"\[url=(?P<href>[^\]]+)\]\[img\](?P<src>[^\[]+)\[/img\]\[/url\]",
            r'<a href="\g<href>" target="_blank"><img src="\g<src>" width="220"></a>',
            desc,
            flags=re.IGNORECASE
        )

        # [img=200]...[/img] (no [url])
        desc = re.sub(
            r"\[img=(?P<width>\d+)\](?P<src>[^\[]+)\[/img\]",
            r'<img src="\g<src>" width="\g<width>">',
            desc,
            flags=re.IGNORECASE
        )

        with open(final_desc_path, 'w', encoding='utf-8') as f:
            f.write(desc)

        return desc.encode("utf-8")

    def get_type_id(self, meta):
        if meta.get('anime'):
            return '44'
        category = meta['category']

        if category == 'MOVIE':
            return '19'

        elif category == 'TV':
            return '7'

        else:
            raise UploadException("Unrecognized category.")

    def file_information(self, meta):
        vc = meta.get('video_codec', '')
        if vc:
            self.video_codec = vc.strip().lower()

        ve = meta.get('video_encode', '')
        if ve:
            self.video_encode = ve.strip().lower()

        vs = meta.get('source', '')
        if vs:
            self.video_source = vs.strip().lower()

        vt = meta.get('type', '')
        if vt:
            self.video_type = vt.strip().lower()

    def movie_type(self, meta):
        # Possible values: "XviD", "DVDR", "x264", "x265", "MP4", "VCD"

        if self.video_source == 'dvd':
            return "DVDR"

        if self.video_codec == 'hevc':
            return "x265"
        else:
            return "x264"

    def tv_type(self, meta):
        # Possible values: "XviD", "HR-XviD", "x264-SD", "x264-HD", "x265-SD", "x265-HD", "Web-SD", "Web-HD", "DVDR", "MP4"

        if self.video_source == 'dvd':
            return "DVDR"

        if self.video_source == 'web':
            if meta.get('sd'):
                return "Web-SD"
            else:
                return "Web-HD"

        if self.video_codec == 'hevc':
            if meta.get('sd'):
                return "x265-SD"
            else:
                return "x265-HD"
        else:
            if meta.get('sd'):
                return "x264-SD"
            else:
                return "x264-HD"

    def anime_type(self, meta):
        # Possible values: "TVSeries", "TVSpecial", "Movie", "OVA", "ONA", "DVDSpecial"
        if meta.get('tvmaze_episode_data', {}).get('season_number') == 0:
            return "TVSpecial"

        if self.video_source == 'dvd':
            return "DVDSpecial"

        category = meta['category']

        if category == 'TV':
            return "TVSeries"

        if category == 'MOVIE':
            return "Movie"

    def movie_source(self, meta):
        # Possible values: "DVD", "DVDSCR", "Workprint", "TeleCine", "TeleSync", "CAM", "BluRay", "HD-DVD", "HDTV", "R5", "WebRIP"

        mapping = {
            "dvd": "DVD",
            "dvdscr": "DVDSCR",
            "workprint": "Workprint",
            "telecine": "TeleCine",
            "telesync": "TeleSync",
            "cam": "CAM",
            "bluray": "BluRay",
            "blu-ray": "BluRay",
            "hd-dvd": "HD-DVD",
            "hdtv": "HDTV",
            "r5": "R5",
            "web": "WebRIP",
            "webrip": "WebRIP"
        }

        src = (self.video_source or "").strip().lower()
        return mapping.get(src, None)

    def tv_source(self, meta):
        # Possible values: "HDTV", "DSR", "PDTV", "TV", "DVD", "DvdScr", "BluRay", "WebRIP"

        mapping = {
            "hdtv": "HDTV",
            "dsr": "DSR",
            "pdtv": "PDTV",
            "tv": "TV",
            "dvd": "DVD",
            "dvdscr": "DvdScr",
            "bluray": "BluRay",
            "blu-ray": "BluRay",
            "web": "WebRIP",
            "webrip": "WebRIP"
        }

        src = (self.video_source or "").strip().lower()
        return mapping.get(src, None)

    def anime_source(self, meta):
        # Possible values: "DVD", "BluRay", "Anime Series", "HDTV"

        mapping = {
            "hdtv": "HDTV",
            "tv": "HDTV",
            "dvd": "DVD",
            "bluray": "BluRay",
            "blu-ray": "BluRay",
            "web": "Anime Series",
            "webrip": "Anime Series"
        }

        src = (self.video_source or "").strip().lower()
        return mapping.get(src, None)

    def anime_v_dar(self, meta):
        # Possible values: "16_9", "4_3"
        if meta.get('is_disc') != "BDMV":
            tracks = meta.get('mediainfo', {}).get('media', {}).get('track', [])
            for track in tracks:
                if track.get('@type') == "Video":
                    dar_str = track.get('DisplayAspectRatio')
                    if dar_str:
                        try:
                            dar = float(dar_str)
                            return "16_9" if dar > 1.34 else "4_3"
                        except (ValueError, TypeError):
                            return "16_9"

            return "16_9"
        else:
            return "16_9"

    def anime_v_codec(self, meta):
        # Possible values: "x264", "h264", "XviD", "DivX", "WMV", "VC1"

        if self.video_codec == 'vc-1':
            return "VC1"

        if self.video_encode == 'h.264':
            return "h264"
        else:
            return 'x264'

    def edit_name(self, meta):
        is_scene = bool(meta.get('scene_name'))
        torrent_name = meta['scene_name'] if is_scene else meta['name']

        name = torrent_name.replace(':', '-')
        name = unicodedata.normalize("NFKD", name)
        name = name.encode("ascii", "ignore").decode("ascii")
        name = re.sub(r'[\\/*?"<>|]', '', name)

        return name

    async def languages(self, meta):
        if not meta.get('subtitle_languages') or meta.get('audio_languages'):
            await process_desc_language(meta, desc=None, tracker=self.tracker)

        lang_map = {
            'english': 'en',
            'japanese': 'jp',
            'korean': 'kr',
            'thai': 'th',
            'chinese': 'zh',
        }

        anime_a_codec = []
        anime_a_ch = []
        anime_a_lang = []

        anime_s_format = []
        anime_s_type = []
        anime_s_lang = []

        audio_languages = meta.get('audio_languages', [])
        if audio_languages:
            audio_desc = meta.get('audio', '').lower()
            found_codec = '0'
            codec_options = {
                'aac': 'aac', 'ac3': 'ac3', 'dd': 'ac3', 'dolby digital': 'ac3', 'ogg': 'ogg', 'mp3': 'mp3',
                'dts-es': 'dtses', 'dtses': 'dtses', 'dts': 'dts', 'flac': 'flac', 'pcm': 'pcm', 'wma': 'wma'
            }
            for key, value in codec_options.items():
                if key in audio_desc:
                    found_codec = value
                    break

            channels_desc = meta.get('channels', '')
            channel_map = {
                '2.0': '2',
                '5.1': '5_1',
                '7.1': '7_1'
            }
            found_channel = channel_map.get(channels_desc, '0')

            for lang_str in audio_languages:
                lang_code = lang_map.get(lang_str.lower(), '1')

                anime_a_codec.append(found_codec)
                anime_a_ch.append(found_channel)
                anime_a_lang.append(lang_code)

        subtitle_languages = meta.get('subtitle_languages', [])
        if subtitle_languages:
            subtitle_format = 'srt'
            subtitle_type = 'sub'

            for lang_str in subtitle_languages:
                lang_code = lang_map.get(lang_str.lower(), '1')

                anime_s_format.append(subtitle_format)
                anime_s_type.append(subtitle_type)
                anime_s_lang.append(lang_code)

        return {
            'anime_a_codec': anime_a_codec,
            'anime_a_ch': anime_a_ch,
            'anime_a_lang': anime_a_lang,
            'anime_s_format': anime_s_format,
            'anime_s_type': anime_s_type,
            'anime_s_lang': anime_s_lang,
        }

    async def get_poster(self, meta):
        poster_url = meta.get('poster')

        poster_file = None
        if poster_url:
            async with httpx.AsyncClient() as client:
                response = await client.get(poster_url)
                if response.status_code == 200:
                    poster_ext = os.path.splitext(poster_url)[1] or ".jpg"
                    poster_filename = f"{meta.get('name')}{poster_ext}"
                    poster_file = (poster_filename, response.content, "image/jpeg")

                    return poster_file

    def get_nfo(self, meta):
        nfo_dir = os.path.join(meta['base_dir'], "tmp", meta['uuid'])
        nfo_files = glob.glob(os.path.join(nfo_dir, "*.nfo"))

        if nfo_files:
            nfo_path = nfo_files[0]

            return {
                'nfo': (
                    os.path.basename(nfo_path),
                    open(nfo_path, "rb"),
                    "application/octet-stream"
                )
            }
        return {}

    async def fetch_data(self, meta, disctype):
        languages = await self.languages(meta)
        self.file_information(meta)

        data = {
            'MAX_FILE_SIZE': 10000000,
            'type': self.get_type_id(meta),
            'tags': '',
            'descr': await self.generate_description(meta),
        }

        if meta.get('anime'):
            data.update({
                'anime_type': self.anime_type(meta),
                'anime_source': self.anime_source(meta),
                'anime_container': 'mkv',
                'anime_v_res': meta.get('resolution'),
                'anime_v_dar': self.anime_v_dar(meta),
                'anime_v_codec': self.anime_v_codec(meta),
                'anime_a_codec[]': ['0'] + languages.get('anime_a_codec'),
                'anime_a_ch[]': ['0'] + languages.get('anime_a_ch'),
                'anime_a_lang[]': ['0'] + languages.get('anime_a_lang'),
                'anime_s_format[]': ['0'] + languages.get('anime_s_format'),
                'anime_s_type[]': ['0'] + languages.get('anime_s_type'),
                'anime_s_lang[]': ['0'] + languages.get('anime_s_lang'),
                })

        else:
            if meta['category'] == 'MOVIE':
                data.update({
                    'movie_type': self.movie_type(meta),
                    'movie_source': self.movie_source(meta),
                    'movie_imdb': f"https://www.imdb.com/title/{meta.get('imdb_info', {}).get('imdbID', '')}",
                    'pack': 0,
                    })

            if meta['category'] == 'TV':
                data.update({
                    'tv_type': self.tv_type(meta),
                    'tv_source': self.tv_source(meta),
                    'tv_imdb': f"https://www.imdb.com/title/{meta.get('imdb_info', {}).get('imdbID', '')}",
                    'pack': 1 if meta.get('tv_pack') else 0,
                    })

        return data

    async def upload(self, meta, disctype):
        await self.validate_credentials(meta)
        await self.edit_torrent(meta, self.tracker, self.source_flag)
        data = await self.fetch_data(meta, disctype)
        requests = await self.get_requests(meta)
        status_message = ''

        if not meta.get('debug', False):
            torrent_id = ''
            upload_url = f"{self.base_url}/takeupload.php"
            torrent_path = f"{meta['base_dir']}/tmp/{meta['uuid']}/[{self.tracker}].torrent"

            with open(torrent_path, 'rb') as torrent_file:
                files = {
                    'file': (f"{self.edit_name(meta)}.torrent", torrent_file, "application/x-bittorrent"),
                }
                files['poster'] = await self.get_poster(meta)
                nfo = self.get_nfo(meta)
                if nfo:
                    files['nfo'] = nfo['nfo']

                response = await self.session.post(upload_url, data=data, files=files, timeout=30)

                if response.status_code == 302:
                    status_message = 'Torrent uploaded successfully.'
                    # Find the torrent id
                    match = re.search(r'details\.php\?id=(\d+)', response.text)
                    if match:
                        torrent_id = match.group(1)
                        meta['tracker_status'][self.tracker]['torrent_id'] = torrent_id

                    if requests:
                        status_message += ' Your upload may fulfill existing requests, check prior console logs.'

                else:
                    status_message = 'The upload appears to have failed. It may have uploaded, go check.'

                    response_save_path = f"{meta['base_dir']}/tmp/{meta['uuid']}/[{self.tracker}]FailedUpload.html"
                    with open(response_save_path, "w", encoding="utf-8") as f:
                        f.write(response.text)
                    console.print(f"Upload failed, HTML response was saved to: {response_save_path}")

                    meta['skipping'] = f"{self.tracker}"
                    return

            await asyncio.sleep(5)  # the tracker takes a while to register the hash
            await self.add_tracker_torrent(meta, self.tracker, self.source_flag, self.announce, self.torrent_url + torrent_id)

        else:
            console.print(data)
            status_message = 'Debug mode enabled, not uploading'

        meta['tracker_status'][self.tracker]['status_message'] = status_message
