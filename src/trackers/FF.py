# -*- coding: utf-8 -*-
import httpx
import os
import platform
import re
from .COMMON import COMMON
from bs4 import BeautifulSoup
from src.console import console
from src.exceptions import UploadException
from pymediainfo import MediaInfo


class FF(COMMON):
    def __init__(self, config):
        super().__init__(config)
        self.tracker = "FF"
        self.banned_groups = [""]
        self.source_flag = "FunFile"
        self.base_url = "https://www.funfile.org"
        self.torrent_url = "https://www.funfile.org/details.php?id="
        self.announce = self.config['TRACKERS'][self.tracker]['announce_url']
        self.auth_token = None
        self.session = httpx.AsyncClient(headers={
            'User-Agent': f"Audionut's Upload Assistant ({platform.system()} {platform.release()})"
        }, timeout=60.0)
        self.signature = "[center][url=https://github.com/Audionut/Upload-Assistant]Created by Audionut's Upload Assistant[/url][/center]"

    async def validate_credentials(self, meta):
        self.cookie_file = os.path.abspath(f"{meta['base_dir']}/data/cookies/{self.tracker}.txt")
        if not os.path.exists(self.cookie_file):
            await self.login(meta)

        test_url = f"{self.base_url}/upload.php"
        try:
            self.session.cookies.update(await self.parseCookieFile(self.cookie_file))
            response = await self.session.get(test_url, timeout=10)

            if response.status_code == 200 and 'login.php' not in str(response.url):
                return True
            else:
                await self.login(meta)
                response = await self.session.get(test_url, timeout=10)
                if response.status_code == 200 and 'login.php' not in str(response.url):
                    return True
                else:
                    return False
        except Exception as e:
            console.print(f"[bold red]{self.tracker}: Error validating credentials: {e}[/bold red]")
            return False

    async def login(self, meta):
        login_url = "https://www.funfile.org/takelogin.php"
        self.cookie_file = os.path.abspath(f"{meta['base_dir']}/data/cookies/{self.tracker}.txt")

        payload = {
            "returnto": "/index.php",
            "username": self.config['TRACKERS'][self.tracker]['username'],
            "password": self.config['TRACKERS'][self.tracker]['password'],
            "login": "Login"
        }

        print(f"[{self.tracker}] Trying to login...")
        response = await self.session.post(login_url, data=payload)

        if response.status_code == 302:
            print(f"[{self.tracker}] Login Successful!")

            with open(self.cookie_file, "w") as f:
                f.write("# Netscape HTTP Cookie File\n")
                f.write("# This file was generated by an automated script.\n\n")
                for cookie in self.session.cookies.jar:
                    domain = cookie.domain
                    include_subdomains = "TRUE" if domain.startswith('.') else "FALSE"
                    path = cookie.path
                    secure = "TRUE" if cookie.secure else "FALSE"
                    expires = str(int(cookie.expires)) if cookie.expires else "0"
                    name = cookie.name
                    value = cookie.value
                    f.write(f"{domain}\t{include_subdomains}\t{path}\t{secure}\t{expires}\t{name}\t{value}\n")
            print(f"[{self.tracker}] Saving the cookie file...")
        else:
            print(f"[{self.tracker}] Login failed. Status code: {response.status_code}")

    async def search_existing(self, meta, disctype):
        if meta['category'] == 'MOVIE':
            query = meta['title']
        if meta['category'] == 'TV':
            query = f"{meta['title']} {meta.get('season', '')}{meta.get('episode', '')}"

        search_url = f"{self.base_url}/suggest.php?q={query}"
        response = await self.session.get(search_url)
        console.print(search_url)
        console.print(response.text)

        if response.status_code == 200 and 'suggest.php' in str(response.url):
            items = [line.strip() for line in response.text.splitlines() if line.strip()]
            return items

        return []

    async def generate_description(self, meta):
        base_desc_path = f"{meta['base_dir']}/tmp/{meta['uuid']}/DESCRIPTION.txt"
        final_desc_path = f"{meta['base_dir']}/tmp/{meta['uuid']}/[{self.tracker}]DESCRIPTION.txt"

        description_parts = []

        # MediaInfo/BDInfo
        tech_info = ""
        if meta.get('is_disc') != 'BDMV':
            video_file = meta['filelist'][0]
            mi_template = os.path.abspath(f"{meta['base_dir']}/data/templates/MEDIAINFO.txt")
            if os.path.exists(mi_template):
                try:
                    media_info = MediaInfo.parse(video_file, output="STRING", full=False, mediainfo_options={"inform": f"file://{mi_template}"})
                    tech_info = str(media_info)
                except Exception:
                    console.print("[bold red]Couldn't find the MediaInfo template[/bold red]")
                    mi_file_path = f"{meta['base_dir']}/tmp/{meta['uuid']}/MEDIAINFO_CLEANPATH.txt"
                    if os.path.exists(mi_file_path):
                        with open(mi_file_path, 'r', encoding='utf-8') as f:
                            tech_info = f.read()
            else:
                console.print("[bold yellow]Using normal MediaInfo for the description.[/bold yellow]")
                mi_file_path = f"{meta['base_dir']}/tmp/{meta['uuid']}/MEDIAINFO_CLEANPATH.txt"
                if os.path.exists(mi_file_path):
                    with open(mi_file_path, 'r', encoding='utf-8') as f:
                        tech_info = f.read()
        else:
            bd_summary_file = f"{meta['base_dir']}/tmp/{meta['uuid']}/BD_SUMMARY_00.txt"
            if os.path.exists(bd_summary_file):
                with open(bd_summary_file, 'r', encoding='utf-8') as f:
                    tech_info = f.read()

        if tech_info:
            description_parts.append(tech_info)

        if os.path.exists(base_desc_path):
            with open(base_desc_path, 'r', encoding='utf-8') as f:
                manual_desc = f.read()
            description_parts.append(manual_desc)

        # Screenshots
        images = meta.get('image_list', [])
        if not images or len(images) < 3:
            raise UploadException("[red]HDS requires at least 3 screenshots.[/red]")

        screenshots_block = "[center][b]Screenshots[/b]\n\n"
        for image in images:
            img_url = image['img_url']
            web_url = image['web_url']
            screenshots_block += f"[url={web_url}][img]{img_url}[/img][/url] "
        screenshots_block += "[/center]"

        description_parts.append(screenshots_block)

        if self.signature:
            description_parts.append(self.signature)

        final_description = "\n\n".join(filter(None, description_parts))
        from src.bbcode import BBCODE
        bbcode = BBCODE()
        desc = final_description
        desc = desc.replace("[user]", "").replace("[/user]", "")
        desc = desc.replace("[align=left]", "").replace("[/align]", "")
        desc = desc.replace("[right]", "").replace("[/right]", "")
        desc = desc.replace("[align=right]", "").replace("[/align]", "")
        desc = desc.replace("[sup]", "").replace("[/sup]", "")
        desc = desc.replace("[sub]", "").replace("[/sub]", "")
        desc = desc.replace("[alert]", "").replace("[/alert]", "")
        desc = desc.replace("[note]", "").replace("[/note]", "")
        desc = desc.replace("[hr]", "").replace("[/hr]", "")
        desc = desc.replace("[h1]", "[u][b]").replace("[/h1]", "[/b][/u]")
        desc = desc.replace("[h2]", "[u][b]").replace("[/h2]", "[/b][/u]")
        desc = desc.replace("[h3]", "[u][b]").replace("[/h3]", "[/b][/u]")
        desc = desc.replace("[ul]", "").replace("[/ul]", "")
        desc = desc.replace("[ol]", "").replace("[/ol]", "")
        desc = desc.replace("[hide]", "").replace("[/hide]", "")
        desc = re.sub(r"\[center\]\[spoiler=.*? NFO:\]\[code\](.*?)\[/code\]\[/spoiler\]\[/center\]", r"NFO:[code][pre]\1[/pre][/code]", desc, flags=re.DOTALL)
        desc = re.sub(r"(\[img=\d+)]", "[img]", desc, flags=re.IGNORECASE)
        desc = bbcode.convert_comparison_to_centered(desc, 1000)
        desc = bbcode.remove_spoiler(desc)

        with open(final_desc_path, 'w', encoding='utf-8') as f:
            f.write(desc)

    def get_type_id(self, meta):
        return

    def movie_type(self, meta):
        return

    def movie_source(self, meta):
        return

    async def gather_data(self, meta, disctype):
        await self.validate_credentials(meta)

        data = {}

        data.update({
            'MAX_FILE_SIZE': 10000000,
            'type': self.get_type_id(meta),
            'movie_type': self.movie_type(meta),
            'movie_source': self.movie_source(meta),
            'movie_imdb': meta.get('imdb_info', {}).get('imdbID', ''),
            'pack': 0,
            'tags': '',
            'descr': await self.generate_description(meta),
            })

        # File: torrent, poster,

        return data

    async def upload(self, meta, disctype):
        data = await self.gather_data(meta, disctype)
        await self.edit_torrent(meta, self.tracker, self.source_flag)
        status_message = ''

        if not meta.get('debug', False):
            torrent_id = ''
            upload_url = f"{self.base_url}/upload.php"
            torrent_path = f"{meta['base_dir']}/tmp/{meta['uuid']}/[{self.tracker}].torrent"

            with open(torrent_path, 'rb') as torrent_file:
                files = {'file_input': (f"{self.tracker}.placeholder.torrent", torrent_file, "application/x-bittorrent")}

                response = await self.session.post(upload_url, data=data, files=files, timeout=120)
                soup = BeautifulSoup(response.text, 'html.parser')

                if 'Clique em baixar para entrar de' in response.text:
                    page_element = soup.select_one('div#wrapper div#content p font')

                    # Find the torrent id
                    match = re.search(r'action=download&id=(\d+)', response.text)
                    if match:
                        torrent_id = match.group(1)
                        meta['tracker_status'][self.tracker]['torrent_id'] = torrent_id

                else:
                    page_element = soup.select_one("div.thin p[style*='color: red']")

                    response_save_path = f"{meta['base_dir']}/tmp/{meta['uuid']}/[{self.tracker}]FailedUpload.html"
                    with open(response_save_path, "w", encoding="utf-8") as f:
                        f.write(response.text)
                    console.print(f"Falha no upload, a resposta HTML foi salva em: {response_save_path}")
                    meta['tracker_status'][self.tracker]['skipped'] = True
                    meta['tracker_status'][self.tracker]['upload'] = False

                # Find the response text
                page_message = ""
                if page_element:
                    page_message = page_element.get_text(strip=True)
                    status_message = page_message

            await self.add_tracker_torrent(meta, self.tracker, self.source_flag, self.announce, self.torrent_url + torrent_id)

        else:
            console.print(data)
            status_message = 'Debug mode enabled, not uploading.'

        meta['tracker_status'][self.tracker]['status_message'] = status_message
